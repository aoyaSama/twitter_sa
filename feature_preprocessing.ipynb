{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Sentiment Classification of Tweets\n",
    "## Feature Preprocessing/Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data\n",
    "Import the raw twitter data for comparison between custom feature engineered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ftfy import fix_encoding\n",
    "\n",
    "\n",
    "def load_data(filename):\n",
    "    \"\"\"Load data from CSV file .\n",
    "\n",
    "    Args:\n",
    "        filename (str): filename/filepath of csv\n",
    "    \"\"\"\n",
    "    # load dataset\n",
    "    data = pd.read_csv(filename)\n",
    "    data.columns = ['sentiment', 'tweet_id', 'tweet']\n",
    "\n",
    "    # Split dataset into features, target labels, and tweet id\n",
    "    # load train features\n",
    "    features = data['tweet']\n",
    "\n",
    "    # fixes encoding for emoji's for feature engineering\n",
    "    features = features.apply(fix_encoding)\n",
    "\n",
    "    # load labels\n",
    "    labels = data['sentiment']\n",
    "\n",
    "    # load tweet_id\n",
    "    tweet_id = data['tweet_id']\n",
    "\n",
    "    return features, labels, tweet_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# load full training data\n",
    "full_train_f_raw, full_train_l, train_id = load_data(\"data/train_full.csv\")\n",
    "\n",
    "# load full development data\n",
    "full_dev_f_raw, full_dev_l, dev_id = load_data(\"data/dev_full.csv\")\n",
    "\n",
    "# load full test data\n",
    "full_test_f_raw, full_test_l, test_id = load_data(\"data/test_full.csv\")\n",
    "\n",
    "full_train_l = np.array(full_train_l)\n",
    "full_dev_l = np.array(full_dev_l)\n",
    "full_test_l = np.array(full_test_l)\n",
    "\n",
    "assert len(full_train_f_raw)==len(full_train_l)\n",
    "assert len(full_dev_f_raw)==len(full_dev_l)\n",
    "assert len(full_test_f_raw)==len(full_test_l)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the tweets / Feature Engineering\n",
    "Data preprocessing is one of the critical steps in any machine learning project. It includes cleaning and formatting the data before feeding into a machine learning algorithm. For NLP, the preprocessing steps are comprised of the following tasks:\n",
    "* Clean the tweets by removing:\n",
    "  * lowercasing\n",
    "  * urls & hyperlinks e.g. https, www, http\n",
    "  * remove user @ reference\n",
    "  * remove standalone #\n",
    "  * remove numeric terms\n",
    "* Tokenising the tweet\n",
    "* Replace emoticons & emoji's with word/meaning\n",
    "* Removing stop words (optional)\n",
    "* Removing punctuations\n",
    "* Lemmatise the tweet\n",
    "* Reduce words with extra characters at the end such as 'happyyyy' to 'happy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# clean the tweets\n",
    "def clean_tweet(tweet):\n",
    "    \"\"\"Clean up a tweet .\n",
    "\n",
    "    Args:\n",
    "        tweet (str): tweet\n",
    "\n",
    "    Returns:\n",
    "        str: cleaned tweet\n",
    "    \"\"\"\n",
    "    # set the tweet to lower\n",
    "    tweet.lower()\n",
    "\n",
    "    # remove urls and hyperlinks\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags=re.MULTILINE)\n",
    "\n",
    "    # remove user @ references\n",
    "    tweet = re.sub(r'\\@\\w+','', tweet)\n",
    "\n",
    "    # remove hash # sign from the word but don't remove the text after the #\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "\n",
    "    # # remove numeric terms\n",
    "    tweet = re.sub(r'[0-9]', '', tweet)\n",
    "\n",
    "    return tweet.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions to replace emoticons and emoji to their description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import spacy\n",
    "from emot.emo_unicode import EMOTICONS_EMO\n",
    "\n",
    "\n",
    "def replace_emoticons(tweet, enable_emo):\n",
    "    \"\"\"Replace all emoticons in a tweet with their respective description.\n",
    "\n",
    "    Args:\n",
    "        tweet ([str]): tokenised tweet\n",
    "\n",
    "    Returns:\n",
    "        [str]: tokenised tweet that has been cleaned\n",
    "    \"\"\"\n",
    "\n",
    "    if(enable_emo is False):\n",
    "        # filter out all emoticons either when it's uppercase or lower\n",
    "        tweet = [token for token in tweet\n",
    "                    if token.upper() not in EMOTICONS_EMO\n",
    "                        or token not in EMOTICONS_EMO]\n",
    "\n",
    "    for token in tweet:\n",
    "        # change to upper for emoticons such as XD, :O, :D\n",
    "        if token.upper() in EMOTICONS_EMO:\n",
    "            emo_word = re.sub(r'[^\\w|s+]', \"\", EMOTICONS_EMO[token.upper()])\n",
    "            tweet[tweet.index(token)] = emo_word.lower()\n",
    "\n",
    "        # check lowercase one too such as :c, :o, :x\n",
    "        elif token in EMOTICONS_EMO:\n",
    "            emo_word = re.sub(r'[^\\w|s+]', \"\", EMOTICONS_EMO[token])\n",
    "            tweet[tweet.index(token)] = emo_word.lower()\n",
    "\n",
    "    return tweet\n",
    "\n",
    "\n",
    "def replace_emojis(tweet_doc, enable_emo):\n",
    "    \"\"\"Replace all emoji in a tweet with their respective description.\n",
    "\n",
    "    Args:\n",
    "        tweet_doc ([type]): [description]\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    # copy the list of tokens into list of strings\n",
    "    tweet  = [i.text for i in tweet_doc]\n",
    "\n",
    "    # return tweet if no emoji\n",
    "    if tweet_doc._.has_emoji is False:\n",
    "        return tweet\n",
    "\n",
    "    for i in range(len(tweet_doc)):\n",
    "        # replace tweet with description of emoji if emoji is enabled\n",
    "        # else remove the emoji\n",
    "        if(tweet_doc[i]._.is_emoji and enable_emo):\n",
    "            tweet[i] = tweet_doc[i]._.emoji_desc.replace(' ', '')\n",
    "        elif(tweet_doc[i]._.is_emoji):\n",
    "            tweet.remove(tweet_doc[i].text)\n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import string\n",
    "from emot.emo_unicode import EMOTICONS_EMO\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from emot.emo_unicode import EMOTICONS_EMO\n",
    "\n",
    "# set the stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def preprocess_tweets(tweets, enable_emo=True, enable_stopwords=False):\n",
    "    \"\"\"Preprocess Tweet (feature enginerring).\n",
    "\n",
    "    Args:\n",
    "        tweets (pd.Dataframe)\n",
    "        enable_emo (bool): Determines if tweets will process emojis and emoticons\n",
    "\n",
    "    Returns:\n",
    "        [str]: preprocessed tweets\n",
    "    \"\"\"\n",
    "\n",
    "    # clean the tweet\n",
    "    tweets = tweets.apply(clean_tweet)\n",
    "\n",
    "    replacement = []\n",
    "\n",
    "    # load the spacymoji nlp pipeline\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    nlp.add_pipe(\"emoji\", first=True)\n",
    "\n",
    "    # add tweets into the spacy pipeline\n",
    "    for tweet in nlp.pipe(tweets,\n",
    "        disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]):\n",
    "\n",
    "        # replace emojis with description\n",
    "        tweet = replace_emojis(tweet, enable_emo)\n",
    "\n",
    "        # replace emoticons with description\n",
    "        tweet = replace_emoticons(tweet, enable_emo)\n",
    "\n",
    "        # remove stopwords\n",
    "        if(enable_stopwords):\n",
    "            tweet = [w for w in tweet if not w in stop_words]\n",
    "\n",
    "        # translator for replacing punctuation also adding empty space as punct\n",
    "        translator = str.maketrans('', '', string.punctuation+' ')\n",
    "\n",
    "        # remove punctuations\n",
    "        tweet = [w.translate(translator) for w in tweet]\n",
    "        tweet = list(filter(None, tweet))\n",
    "\n",
    "        # remove single characters from tweet\n",
    "        tweet = [w for w in tweet if len(w)>1]\n",
    "\n",
    "        # lemmatise tweet\n",
    "        lem = WordNetLemmatizer()\n",
    "        tweet = [lem.lemmatize(w, pos='a') for w in tweet]\n",
    "\n",
    "        # remove duplicate characters at the end of words such as happyyyyy to happy\n",
    "        tweet = re.sub(r'(.*)\\1{2,}', r'\\1', ' '.join(tweet))\n",
    "\n",
    "        replacement.append(tweet)\n",
    "\n",
    "    return replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished preprocess\n"
     ]
    }
   ],
   "source": [
    "# Preprocess tweets/features on full data\n",
    "train_f_emo = preprocess_tweets(full_train_f_raw)\n",
    "dev_f_emo = preprocess_tweets(full_dev_f_raw)\n",
    "test_f_emo = preprocess_tweets(full_test_f_raw)\n",
    "\n",
    "train_f_no_emo = preprocess_tweets(full_train_f_raw, enable_emo=False)\n",
    "dev_f_no_emo = preprocess_tweets(full_dev_f_raw, enable_emo=False)\n",
    "test_f_no_emo = preprocess_tweets(full_test_f_raw, enable_emo=False)\n",
    "\n",
    "train_f_sw = preprocess_tweets(full_train_f_raw, enable_stopwords=True)\n",
    "dev_f_sw = preprocess_tweets(full_dev_f_raw, enable_stopwords=True)\n",
    "test_f_sw = preprocess_tweets(full_test_f_raw, enable_stopwords=True)\n",
    "\n",
    "print(\"finished preprocess\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "headers = ['sentiment', 'tweet_id', 'tweet']\n",
    "file_exe = '.csv'\n",
    "processed_data = {\n",
    "    'train_f_emo':      [full_train_l, train_id, train_f_emo],\n",
    "    'dev_f_emo':        [full_dev_l, dev_id, dev_f_emo],\n",
    "    'test_f_emo':       [full_test_l, test_id, test_f_emo],\n",
    "\n",
    "    'train_f_no_emo':   [full_train_l, train_id, train_f_no_emo],\n",
    "    'dev_f_no_emo':     [full_dev_l, dev_id, dev_f_emo],\n",
    "    'test_f_no_emo':    [full_test_l, test_id, test_f_no_emo],\n",
    "\n",
    "    'train_f_sw':       [full_train_l, train_id, train_f_sw],\n",
    "    'dev_f_sw':         [full_dev_l, dev_id, dev_f_sw],\n",
    "    'test_f_sw':        [full_test_l, test_id, test_f_sw]\n",
    "}\n",
    "\n",
    "# export each preprocessed data into data\n",
    "for data in processed_data:\n",
    "    filename = 'data/' + data + file_exe\n",
    "    df = pd.DataFrame(np.column_stack(processed_data[data]), columns=headers)\n",
    "    df.to_csv(filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8e9837aefb1e65694c07902c38a155dd3f363cee1560a042c272681141003a13"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
